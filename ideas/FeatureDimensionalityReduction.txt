num_input = number of input neurons
num_output = number of output neurons
num_hidden[L] = number of hidden neurons in layer L

Input: 

all_desc = get_all_descriptors()

# Descriptors which produced at least one verified match
# IMPORTANT: Be careful that there are no singletons in the learning data,
# otherwise you weill negatively weight some possibly good examples
pos_examples = get_descriptors_with_at_least_one_verified_match()

# Descriptors which produced no matches
neg_examples = setdiff( all_desc, pos_examples )


# Notes: 
'''Real Valued numbers can be translated into binary neurons.  A SIFT
descriptor with 128 real numbers becomes 1024 binary neurons with connected
weights.  FREAK is already binary. Cascading can be handled by modeling
parts of the descriptor. Model the coarse part, then the coarse+finer part, then
the whole thing. It is cummulative.
'''

Things which we can do: 

Most of these should be able to be tried with a simple linear SVM with
a simple Neural Network. Try that first then see if nonlinearity 
from the nueral network makes a difference.

* Learn a Projection which reduces feature dimensionality 
neural_net = train_net( all_desc, deep=True, dropout=True, RBM_init=True)

* Learn a measure to compare to descriptors for dinstinctiveness 

* Learn a zebraness measure. 

num_output = 1 # On for zebra, off for not.
neural_net = train_net( pos_examples, neg_examples

* Learn a zebraness measure with some grandularity
Do this by attaching the matching score of each verified match to a known
individual, this will be the data the net tries to model. I guess this
should actually take a pair of descriptors as input, and learn how to 
compare them. This basically learns a weighting of importance on the
descriptors. 

num_output = enough to model a real number. 


* Can learning be done on whole image representations? 
* Can learning be done with convolutional information, or does 
this just add extranious complexity? 

