    '''
Image collection: 11 Oxford landmarks (ie particular part of a building) and distractors.

Landmark images were taken from Flickr [3], using queries such as “Oxford Christ Church” and “Oxford Radcliffe Camera.”
Distractors were taken by seaching on “Oxford” alone.
The entire dataset consists of 5,062 high resolution (1024 × 768) images.

For each landmark we chose 5 different query regions.
The ﬁve queries are used so that retrieval performance can be averaged over any individual query peculiarities.

We obtain ground truth manually by searching over the entire dataset for the 11 landmarks.

Images are assigned one of four possible labels:
    (1) Good – a nice, clear picture of the object/building.
    (2) OK – more than 25% of the object is clearly visible.
    (3) Junk – less than 25% of the object is visible, or there is a very high level of occlusion or distortion. 
    (4) Absent – the object is not present. 
    
    The number of occurrences of the different landmarks range between 7 and 220 good and ok images.
    
In addition to this labelled set, we use two other datasets to stress-test
retrieval performance when scaling up. These consist of images crawled from
Flickr’s list of most popular tags. The images in our datasets will not in
general be disjoint when crawled from Flickr, so we remove exact duplicates from the sets. 

We then assume that these datasets contain no occurrences of the objects being
searched for, so they act as distractors, testing both the performance and
scalability of our system. 


100K dataset: Crawled from Flickr's 145 most popular tags and
consists of 99,782 high resolution (1024 × 768) images.  

1M dataset. Crawled from Flickr's 450 most popular tags and
consists of 1,040,801 medium resolution (500 × 333) images.

Table 1: 
 _____________________________________________________
|Dataset # images     # features   Size of descriptors|
|-----------------------------------------------------|
|5K         5,062     16,334,970                1.9 GB|
|100K      99,782    277,770,833               33.1 GB|
|1M     1,040,801  1,186,469,709              141.4 GB|
|-----------------------------------------------------|
|Total  1,145,645  1,480,575,512              176.4 GB|
|-----------------------------------------------------|

To evaluate the performance we use the average precision (AP) measure computed
as the area under the precision-recall curve for a query. 

Precision is defined as the ratio of retrieved positive images to the total number retrieved.

Recall is defined as the ratio of the number of retrieved positive images to the total
number of positive images in the corpus. 

We compute an average precision score for each of the 5 queries for a landmark,
averaging these to obtain a mean Average Precision (mAP) score.

The average of these mAP scores is used as a single number to evaluate the
overall performance. 

In computing the average precision, we use the Good and Ok images as positive
examples of the landmark in question, Absent images as negative examples and
Junk images as null examples. These null examples are treated as though they are
not present in the database – our score is unaffected whether they are returned
or not.

'''

    '''
    We ﬁnd that the two-way transfer error with scale threshold performs the
    best on the data. In cases where a simpler one-way transfer error sufﬁces,
    we can speed up veriﬁcation when there is a high visual-word
    multiplicity between the two images (usually for smaller vocabularies). We
    merely need to ﬁnd the spatially closest matching feature in the target to a
    particular query feature and check whether this single distance is less than
    the threshold. This is done using a 2D k-d tree, to provide logarithmic
    time search. In all experiments where “spatial” is speciﬁed, we have
    used our spatial veriﬁcation procedure to re-rank up to the top 1000
    images. 
    
    We consider spatial veriﬁcation to be “successful” if we ﬁnd a
    transformation with at least 4 inlier correspondences. We re-rank the
    images by scoring them equal to the sum of the idf values for the inlier
    words andh place spatially veriﬁed images above unveriﬁed ones in the
    ranking. 
    
    We abort the re-ranking early (after considering fewer than 1000
    images) if we process 20 images in a row without a successful spatial
    veriﬁcation. We ﬁnd this gives a good trade-off between speed and accuracy
    '''


def compute_average_precision(res, k):
    '''
    % from wikipedia: http://en.wikipedia.org/wiki/Information_retrieval#Mean_average_precision
    $p(r)$ = precision as a function of recall $r$
    $AveP$ = $\int_0^1 p(r) dr$ = $\sum_{k=1}^n P(k) \del r(k)$

    $k$ is the rank in sequence of retrieved documents
    $n$ is the number of retrieved documents

    $\del r(k) = r(k) - r(k-1)$ = change in recall 
    '''
    pass

def compute_mean_average_precision(res, k):
    '''
    MAP = 1/Q \sum_{q=1}^Q AveP(q) 
    '''
